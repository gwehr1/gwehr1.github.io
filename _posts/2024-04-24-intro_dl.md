# First Intro to Practical Deep Learning
This post is going to detail some of the fundamental concepts that I have learnt in my first taste of a hands-on practical deep-learning example.
The code for the example is in Professor Lovell's [00-is-it-a-bird-creating-a-model-from-your-own-data.ipynb](https://github.com/lovellbrian/course22/blob/master/00-is-it-a-bird-creating-a-model-from-your-own-data.ipynb) Jupyter notebook.
I'd recommend taking a look at it - it focusses on training a model for image classification!
In essence, once trained, a deep learning model acts like a program, taking inputs and delivering outputs based on its learned parameters.

## New Terminology:

+ Epoch: One pass through the entire dataset.
+ Batch: A subset of the dataset processed at one time. Training involves dividing the dataset into batches.
+ Backpropagation: The method by which the model learns during training, adjusting its weights based on the error of its predictions.
+ Validation Set: Typically 20% of the data, withheld to test the model’s performance after each training epoch.
+ Test Set: Completely unseen data used to test the model's actual performance after training is completed
+ Loss Function: A measure of how well the model predicts the correct labels, guiding the model adjustments via gradient descent, a method to find the minimum of the function.

## Training Process:
I've learnt that in deep learning, training a model involves feeding it data to learn from, so that it can refine its ability to predict or classify new, unseen data. 

During training, the model processes each batch of data, calculates the loss (the error in its predictions), and updates itself through backpropagation. We monitor both training loss and validation loss across epochs. Training loss should ideally approach zero, indicating the model fits the training data well. However, if validation loss increases, it suggests overfitting—where the model learns the training data too well, including the noise, diminishing its ability to generalize.

The evolution from simple feature recognition in early layers to complex object identification in deeper layers is pretty awesome to see! The model learns itself what the best features are, to optimise performance. The result is a model capable of tasks that were super challenging not that long ago (2015), such as accurately classifying images into categories like birds, humans, dogs, etc. This capability is built upon increasingly complex layers in the network, each adding to the model's ability to discern more intricate patterns and features.

## Fine-tuning:
Using a pre-trained network like ResNet, a deep learning model can start with sophisticated foundational knowledge (like recognizing basic shapes and colors). We then adapt the final layers, customizing them to classify new types of images (e.g., distinguishing birds from other objects). This adaptation process involves re-training these final layers on the specific tasks we are interested in, enhancing the model’s applicability to more specific complex datasets.
